import pandas as pd
import numpy as np
import re

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Load the emotion_dataset
emotion_dataset = pd.read_csv("surprise_dataset1.csv")

# Preprocessing function for emotion_dataset
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s\-\.!?,]', '', text)
    return text

emotion_dataset['Clean_Text'] = emotion_dataset['Text'].apply(preprocess_text)

# Define the emotion weights
emotion_weights = {
    'surprise': 0.6,
    'shame': 0.9,
    'joy': 0.5,
    'disgust': 0.8,
    'fear': 0.7,
    'anger': 0.7
}

# Create a Tokenizer to index words
tokenizer = Tokenizer()
tokenizer.fit_on_texts(emotion_dataset['Clean_Text'])

X = tokenizer.texts_to_sequences(emotion_dataset['Clean_Text'])
X = pad_sequences(X)

# Convert emotion labels to numerical format using LabelEncoder
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(emotion_dataset['Emotion'])

# Inspect unique emotion labels
unique_emotions = label_encoder.classes_

# Convert emotion labels to one-hot encoded vectors
y_train_one_hot = tf.keras.utils.to_categorical(
    y_train, num_classes=len(unique_emotions))

# Split the dataset into training and testing sets
X_train, X_test, y_train_one_hot, y_test_one_hot = train_test_split(
    X, y_train_one_hot, test_size=0.2, random_state=42)

# Build an LSTM model
lstm_input = Input(shape=(X_train.shape[1],))
embedding_layer_lstm = Embedding(input_dim=len(
    tokenizer.word_index) + 1, output_dim=100)(lstm_input)
lstm_layer = LSTM(64)(embedding_layer_lstm)

# Add a Dense layer for classification
dense_layer = Dense(64, activation='relu')(lstm_layer)
dropout_layer = Dropout(0.5)(dense_layer)

# Update the output layer to match the number of unique emotions
output_layer = Dense(len(unique_emotions), activation='softmax')(dropout_layer)

model = Model(inputs=lstm_input, outputs=output_layer)

model.compile(loss='categorical_crossentropy',
              optimizer='adam', metrics=['accuracy'])

# Print model summary
print(model.summary())

# Train the LSTM model
model.fit(X_train, y_train_one_hot, epochs=10,
          batch_size=32, validation_split=0.1, verbose=1)

# Evaluate the LSTM model on training data
train_loss, train_accuracy = model.evaluate(X_train, y_train_one_hot)
print(f"Training Accuracy: {train_accuracy:.2f}")

# Evaluate the LSTM model on test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)
print(f"Test Accuracy: {test_accuracy:.2f}")

# Map numerical labels back to original emotion categories
predicted_labels = model.predict(X_test)
predicted_emotions = [
    unique_emotions[np.argmax(pred)] for pred in predicted_labels]
true_emotions = [unique_emotions[np.argmax(true)] for true in y_test_one_hot]

# Calculate weighted accuracy for each emotion category
accuracy_by_emotion = {}
for emotion in unique_emotions:
    correct_predictions = [1 for pred, true in zip(
        predicted_emotions, true_emotions) if pred == emotion and true == emotion]
    total_predictions = [1 for true in true_emotions if true == emotion]
    accuracy = sum(correct_predictions) / \
        sum(total_predictions) if sum(total_predictions) > 0 else 0
    accuracy_weighted = accuracy * emotion_weights.get(emotion, 1.0)
    accuracy_by_emotion[emotion] = accuracy_weighted

print("Weighted Accuracy by Emotion:")
for emotion, accuracy in accuracy_by_emotion.items():
    print(f"{emotion}: {accuracy * 100:.2f}%")
